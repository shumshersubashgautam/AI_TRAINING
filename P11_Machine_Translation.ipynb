{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "P11:Machine Translation.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shumshersubashgautam/AI_TRAINING/blob/master/P11_Machine_Translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LedJNyenrH9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 62
        },
        "outputId": "ccf5d165-aae6-48a9-914a-ed104d9d4730"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import math\n",
        "import os"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8ap1o78ohqo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "25cc375f-bea9-43b6-ba33-76e09fb1153b"
      },
      "source": [
        "!git clone https://github.com/Hvass-Labs/TensorFlow-Tutorials.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'TensorFlow-Tutorials'...\n",
            "remote: Enumerating objects: 9, done.\u001b[K\n",
            "remote: Counting objects:  11% (1/9)\u001b[K\rremote: Counting objects:  22% (2/9)\u001b[K\rremote: Counting objects:  33% (3/9)\u001b[K\rremote: Counting objects:  44% (4/9)\u001b[K\rremote: Counting objects:  55% (5/9)\u001b[K\rremote: Counting objects:  66% (6/9)\u001b[K\rremote: Counting objects:  77% (7/9)\u001b[K\rremote: Counting objects:  88% (8/9)\u001b[K\rremote: Counting objects: 100% (9/9)\u001b[K\rremote: Counting objects: 100% (9/9), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 480 (delta 2), reused 5 (delta 2), pack-reused 471\u001b[K\n",
            "Receiving objects: 100% (480/480), 62.52 MiB | 25.55 MiB/s, done.\n",
            "Resolving deltas: 100% (223/223), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukKAuTlVolYI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7b63e17c-7223-491e-8a48-bb2795d79626"
      },
      "source": [
        "cd /content/TensorFlow-Tutorials"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/TensorFlow-Tutorials\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ibb2AQjfn25O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from tf.keras.models import Model  # This does not work!\n",
        "from tensorflow.python.keras.models import Model\n",
        "from tensorflow.python.keras.layers import Input, Dense, GRU, Embedding\n",
        "from tensorflow.python.keras.optimizers import RMSprop\n",
        "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0k3FWjooDTy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "eb90c07e-8fa3-4c70-b8b4-ababe6fb8226"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.15.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWsbNy9yoPLP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "11aa0929-403a-4189-f713-34c59f36497e"
      },
      "source": [
        "tf.keras.__version__"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.2.4-tf'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERu08e3eoUIA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import europarl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TERRqk02ozwv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "language_code='da'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrIoPMV_o3Ip",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mark_start = 'ssss '\n",
        "mark_end = ' eeee'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOhgx2djo6nl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "efb1c5be-52e1-4a18-9d2b-1889c68f34a5"
      },
      "source": [
        "europarl.maybe_download_and_extract(language_code=language_code)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "- Download progress: 100.0%\n",
            "Download finished. Extracting files.\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEqhNqG1o98z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_src = europarl.load_data(english=False,\n",
        "                              language_code=language_code)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PFB9IqvqCMw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_dest = europarl.load_data(english=True,\n",
        "                               language_code=language_code,\n",
        "                               start=mark_start,\n",
        "                               end=mark_end)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJEg_7O4qGL8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "idx = 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2tE4AFXqMeQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "53b2e1fe-adee-4314-b32f-cc5304e01663"
      },
      "source": [
        "data_src[idx]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Som De kan se, indfandt det store \"år 2000-problem\" sig ikke. Til gengæld har borgerne i en del af medlemslandene været ramt af meget forfærdelige naturkatastrofer.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxSVeX_NqOoO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4d5ea9c2-434c-45dc-ae28-350c48d64f48"
      },
      "source": [
        "data_dest[idx]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"ssss Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful. eeee\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EM3KY9CqR-A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "idx = 8002"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGsF2uIPqU4k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "81dc0386-13ad-4371-c35f-f1ead37fa676"
      },
      "source": [
        "data_src[idx]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\"Car il savait ce que cette foule en joie ignorait, et qu\\'on peut lire dans les livres, que le bacille de la peste ne meurt ni ne disparaît jamais, qu\\'il peut rester pendant des dizaines d\\'années endormi dans les meubles et le linge, qu\\'il attend patiemment dans les chambres, les caves, les malles, les mouchoirs et les paperasses, et que, peut-être, le jour viendrait où, pour le malheur et l\\'enseignement des hommes, la peste réveillerait ses rats et les enverrait mourir dans une cité heureuse.\" (Thi han vidste det, som denne glade forsamling ikke vidste, og som man kan læse i bøger, at pestens bacille aldrig dør og aldrig forsvinder, at den kan sove i mange år i møbler og linned, at den venter tålmodigt i kamre, kældre, kufferter, lommetørklæder og papirer, og at den dag måske kommer, hvor pesten til menneskenes skade og oplysning vågner sine rotter og sender dem ud for at dø i en lykkelig by.)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dyz8kfyeqXFy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "57adafca-98c1-4be6-b07d-a7663ed1fbe8"
      },
      "source": [
        "data_dest[idx]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ssss \"He knew what those jubilant crowds did not know but could have learned from books: that the plague bacillus never dies or disappears for good; that it can lie dormant for years and years in furniture and linen-chests; that it bides its time in bedrooms, cellars, trunks, and bookshelves; and that perhaps the day would come when, for the bane and the enlightening of men, it would rouse up its rats again and send them forth to die in a happy city.\" eeee'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4ufJrA8qafX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_words = 10000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKpUDqhSqdSv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TokenizerWrap(Tokenizer):\n",
        "    \"\"\"Wrap the Tokenizer-class from Keras with more functionality.\"\"\"\n",
        "    \n",
        "    def __init__(self, texts, padding,\n",
        "                 reverse=False, num_words=None):\n",
        "        \"\"\"\n",
        "        :param texts: List of strings. This is the data-set.\n",
        "        :param padding: Either 'post' or 'pre' padding.\n",
        "        :param reverse: Boolean whether to reverse token-lists.\n",
        "        :param num_words: Max number of words to use.\n",
        "        \"\"\"\n",
        "\n",
        "        Tokenizer.__init__(self, num_words=num_words)\n",
        "\n",
        "        # Create the vocabulary from the texts.\n",
        "        self.fit_on_texts(texts)\n",
        "\n",
        "        # Create inverse lookup from integer-tokens to words.\n",
        "        self.index_to_word = dict(zip(self.word_index.values(),\n",
        "                                      self.word_index.keys()))\n",
        "\n",
        "        # Convert all texts to lists of integer-tokens.\n",
        "        # Note that the sequences may have different lengths.\n",
        "        self.tokens = self.texts_to_sequences(texts)\n",
        "\n",
        "        if reverse:\n",
        "            # Reverse the token-sequences.\n",
        "            self.tokens = [list(reversed(x)) for x in self.tokens]\n",
        "        \n",
        "            # Sequences that are too long should now be truncated\n",
        "            # at the beginning, which corresponds to the end of\n",
        "            # the original sequences.\n",
        "            truncating = 'pre'\n",
        "        else:\n",
        "            # Sequences that are too long should be truncated\n",
        "            # at the end.\n",
        "            truncating = 'post'\n",
        "\n",
        "        # The number of integer-tokens in each sequence.\n",
        "        self.num_tokens = [len(x) for x in self.tokens]\n",
        "\n",
        "        # Max number of tokens to use in all sequences.\n",
        "        # We will pad / truncate all sequences to this length.\n",
        "        # This is a compromise so we save a lot of memory and\n",
        "        # only have to truncate maybe 5% of all the sequences.\n",
        "        self.max_tokens = np.mean(self.num_tokens) \\\n",
        "                          + 2 * np.std(self.num_tokens)\n",
        "        self.max_tokens = int(self.max_tokens)\n",
        "\n",
        "        # Pad / truncate all token-sequences to the given length.\n",
        "        # This creates a 2-dim numpy matrix that is easier to use.\n",
        "        self.tokens_padded = pad_sequences(self.tokens,\n",
        "                                           maxlen=self.max_tokens,\n",
        "                                           padding=padding,\n",
        "                                           truncating=truncating)\n",
        "\n",
        "    def token_to_word(self, token):\n",
        "        \"\"\"Lookup a single word from an integer-token.\"\"\"\n",
        "\n",
        "        word = \" \" if token == 0 else self.index_to_word[token]\n",
        "        return word \n",
        "\n",
        "    def tokens_to_string(self, tokens):\n",
        "        \"\"\"Convert a list of integer-tokens to a string.\"\"\"\n",
        "\n",
        "        # Create a list of the individual words.\n",
        "        words = [self.index_to_word[token]\n",
        "                 for token in tokens\n",
        "                 if token != 0]\n",
        "        \n",
        "        # Concatenate the words to a single string\n",
        "        # with space between all the words.\n",
        "        text = \" \".join(words)\n",
        "\n",
        "        return text\n",
        "    \n",
        "    def text_to_tokens(self, text, reverse=False, padding=False):\n",
        "        \"\"\"\n",
        "        Convert a single text-string to tokens with optional\n",
        "        reversal and padding.\n",
        "        \"\"\"\n",
        "\n",
        "        # Convert to tokens. Note that we assume there is only\n",
        "        # a single text-string so we wrap it in a list.\n",
        "        tokens = self.texts_to_sequences([text])\n",
        "        tokens = np.array(tokens)\n",
        "\n",
        "        if reverse:\n",
        "            # Reverse the tokens.\n",
        "            tokens = np.flip(tokens, axis=1)\n",
        "\n",
        "            # Sequences that are too long should now be truncated\n",
        "            # at the beginning, which corresponds to the end of\n",
        "            # the original sequences.\n",
        "            truncating = 'pre'\n",
        "        else:\n",
        "            # Sequences that are too long should be truncated\n",
        "            # at the end.\n",
        "            truncating = 'post'\n",
        "\n",
        "        if padding:\n",
        "            # Pad and truncate sequences to the given length.\n",
        "            tokens = pad_sequences(tokens,\n",
        "                                   maxlen=self.max_tokens,\n",
        "                                   padding='pre',\n",
        "                                   truncating=truncating)\n",
        "\n",
        "        return tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXtzYJCyqhgu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a1d80be6-8cbc-4422-df07-d474d7d5e4e9"
      },
      "source": [
        "%%time\n",
        "tokenizer_src = TokenizerWrap(texts=data_src,\n",
        "                              padding='pre',\n",
        "                              reverse=True,\n",
        "                              num_words=num_words)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2min 34s, sys: 601 ms, total: 2min 34s\n",
            "Wall time: 2min 34s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvfwM1oUqkE-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "53946fa4-18f4-4940-d742-ff99dc3de0a4"
      },
      "source": [
        "%%time\n",
        "tokenizer_dest = TokenizerWrap(texts=data_dest,\n",
        "                               padding='post',\n",
        "                               reverse=False,\n",
        "                               num_words=num_words)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1min 55s, sys: 595 ms, total: 1min 56s\n",
            "Wall time: 1min 56s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_Gihm-XrOcV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "fca36300-1dff-44d0-f3f8-d28c1bb423a0"
      },
      "source": [
        "tokens_src = tokenizer_src.tokens_padded\n",
        "tokens_dest = tokenizer_dest.tokens_padded\n",
        "print(tokens_src.shape)\n",
        "print(tokens_dest.shape)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1968800, 47)\n",
            "(1968800, 55)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBnvAFxGsi6c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "15d107d6-30a6-4db1-f03e-d585b89615b4"
      },
      "source": [
        "token_start = tokenizer_dest.word_index[mark_start.strip()]\n",
        "token_start"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIkVMYZIslZL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "49df586b-f31e-46f8-a52c-11df3cc07521"
      },
      "source": [
        "token_end = tokenizer_dest.word_index[mark_end.strip()]\n",
        "token_end"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dEIJffysodd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "idx = 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQxF0RWnsrKj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "ea162a12-be82-4314-8a22-77f01ad086c7"
      },
      "source": [
        "tokens_src[idx]"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 3069,\n",
              "       3374,   43,    7, 1386,  108, 1995,    7,  178,    9,    3,  302,\n",
              "         19, 2076,    8,   20,   39,  285,  499,   69,  136,    5,  166,\n",
              "         24,   10,   13], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "He8sj_RPstox",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "29de0ff9-6250-4b8f-c1b4-0c6ce4e263bf"
      },
      "source": [
        "tokenizer_src.tokens_to_string(tokens_src[idx])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'naturkatastrofer forfærdelige meget af ramt været medlemslandene af del en i borgerne har gengæld til ikke sig problem 2000 år store det se kan de som'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3F5sSVzswRB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6cae1846-5b33-46f8-e3ae-f336d1f3b5df"
      },
      "source": [
        "data_src[idx]"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Som De kan se, indfandt det store \"år 2000-problem\" sig ikke. Til gengæld har borgerne i en del af medlemslandene været ramt af meget forfærdelige naturkatastrofer.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyrWKQLQszDG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "3bd844bd-6d07-43b0-b83c-697ebbe4cb8c"
      },
      "source": [
        "tokens_dest[idx]"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   2,  404,   19,   43,   26,   20,  618,    1, 1451,    5, 9785,\n",
              "        174,    1,   81,    7,    9,  214,    4,   67, 2200,    9, 1596,\n",
              "          4,  892, 1762,    8, 1480,  107, 5494,    3,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ihpvwqq9s1pg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "20f0fb1b-08fd-455a-c40b-ba2edcd89d9c"
      },
      "source": [
        "tokenizer_dest.tokens_to_string(tokens_dest[idx])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ssss although as you will have seen the failed to materialise still the people in a number of countries suffered a series of natural disasters that truly were dreadful eeee'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "---CMmmus4Dl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b49c1753-9a5e-48d2-8cac-89e5ce04af4c"
      },
      "source": [
        "data_dest[idx]"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"ssss Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful. eeee\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvmgsDn0s7ZL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder_input_data = tokens_src"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOQyiFcis9dx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5a8b96b2-a6aa-490c-d272-8ed04a7bbe97"
      },
      "source": [
        "decoder_input_data = tokens_dest[:, :-1]\n",
        "decoder_input_data.shape"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1968800, 54)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwxuZRjhs_2E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "980b1a62-a823-42ca-cea1-0095cc1fec94"
      },
      "source": [
        "decoder_output_data = tokens_dest[:, 1:]\n",
        "decoder_output_data.shape"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1968800, 54)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5f_JKaxgtCQh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "idx = 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ou8tFAiXtEzb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "11cd6434-b709-465a-8b0f-c5b3261825e4"
      },
      "source": [
        "decoder_input_data[idx]"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   2,  404,   19,   43,   26,   20,  618,    1, 1451,    5, 9785,\n",
              "        174,    1,   81,    7,    9,  214,    4,   67, 2200,    9, 1596,\n",
              "          4,  892, 1762,    8, 1480,  107, 5494,    3,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIcnpe08tHGT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "1ae016ed-3f35-47e2-8113-43b8150df299"
      },
      "source": [
        "decoder_output_data[idx]"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 404,   19,   43,   26,   20,  618,    1, 1451,    5, 9785,  174,\n",
              "          1,   81,    7,    9,  214,    4,   67, 2200,    9, 1596,    4,\n",
              "        892, 1762,    8, 1480,  107, 5494,    3,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdWc-HaptJYM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "abe29ce8-7fd9-404e-91ef-c505153d5fe9"
      },
      "source": [
        "tokenizer_dest.tokens_to_string(decoder_input_data[idx])"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ssss although as you will have seen the failed to materialise still the people in a number of countries suffered a series of natural disasters that truly were dreadful eeee'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kK5Sk5ROtLs6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DY2M-IqWtOZR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder_input = Input(shape=(None, ), name='encoder_input')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RTYz_ROtT-P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_size = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIcyjGVCtWT-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "1507f63a-ad7a-48c9-f66c-4d72b016845b"
      },
      "source": [
        "\n",
        "encoder_embedding = Embedding(input_dim=num_words,\n",
        "                              output_dim=embedding_size,\n",
        "                              name='encoder_embedding')"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ew1s1Y56tY8G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "state_size = 512"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mif4yBctblf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder_gru1 = GRU(state_size, name='encoder_gru1',\n",
        "                   return_sequences=True)\n",
        "encoder_gru2 = GRU(state_size, name='encoder_gru2',\n",
        "                   return_sequences=True)\n",
        "encoder_gru3 = GRU(state_size, name='encoder_gru3',\n",
        "                   return_sequences=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cIDyxM4tdmO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def connect_encoder():\n",
        "    # Start the neural network with its input-layer.\n",
        "    net = encoder_input\n",
        "    \n",
        "    # Connect the embedding-layer.\n",
        "    net = encoder_embedding(net)\n",
        "\n",
        "    # Connect all the GRU-layers.\n",
        "    net = encoder_gru1(net)\n",
        "    net = encoder_gru2(net)\n",
        "    net = encoder_gru3(net)\n",
        "\n",
        "    # This is the output of the encoder.\n",
        "    encoder_output = net\n",
        "    \n",
        "    return encoder_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JU-D9HQqtgZc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "5726dd82-43bf-41f6-ba0f-413bd06306f6"
      },
      "source": [
        "encoder_output = connect_encoder()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxJC9BHltkB6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "decoder_initial_state = Input(shape=(state_size,),\n",
        "                              name='decoder_initial_state')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cM9OtG3atoPL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder_input = Input(shape=(None, ), name='decoder_input')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_H5Zdnetqd2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder_embedding = Embedding(input_dim=num_words,\n",
        "                              output_dim=embedding_size,\n",
        "                              name='decoder_embedding')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tfUKqHpts0N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder_gru1 = GRU(state_size, name='decoder_gru1',\n",
        "                   return_sequences=True)\n",
        "decoder_gru2 = GRU(state_size, name='decoder_gru2',\n",
        "                   return_sequences=True)\n",
        "decoder_gru3 = GRU(state_size, name='decoder_gru3',\n",
        "                   return_sequences=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ft5RqE3XtvOO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder_dense = Dense(num_words,\n",
        "                      activation='linear',\n",
        "                      name='decoder_output')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClmgO6mTtyBo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def connect_decoder(initial_state):\n",
        "    # Start the decoder-network with its input-layer.\n",
        "    net = decoder_input\n",
        "\n",
        "    # Connect the embedding-layer.\n",
        "    net = decoder_embedding(net)\n",
        "    \n",
        "    # Connect all the GRU-layers.\n",
        "    net = decoder_gru1(net, initial_state=initial_state)\n",
        "    net = decoder_gru2(net, initial_state=initial_state)\n",
        "    net = decoder_gru3(net, initial_state=initial_state)\n",
        "\n",
        "    # Connect the final dense layer that converts to\n",
        "    # one-hot encoded arrays.\n",
        "    decoder_output = decoder_dense(net)\n",
        "    \n",
        "    return decoder_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJ-CNzrat1V1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder_output = connect_decoder(initial_state=encoder_output)\n",
        "\n",
        "model_train = Model(inputs=[encoder_input, decoder_input],\n",
        "                    outputs=[decoder_output])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1FFKxbdt_zu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_encoder = Model(inputs=[encoder_input],\n",
        "                      outputs=[encoder_output])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlsfT1LkuC6v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder_output = connect_decoder(initial_state=decoder_initial_state)\n",
        "\n",
        "model_decoder = Model(inputs=[decoder_input, decoder_initial_state],\n",
        "                      outputs=[decoder_output])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-xoHkaKuHGW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model_train.compile(optimizer=optimizer,\n",
        "#                     loss='sparse_categorical_crossentropy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVLGKYqxuLt5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sparse_cross_entropy(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculate the cross-entropy loss between y_true and y_pred.\n",
        "    \n",
        "    y_true is a 2-rank tensor with the desired output.\n",
        "    The shape is [batch_size, sequence_length] and it\n",
        "    contains sequences of integer-tokens.\n",
        "\n",
        "    y_pred is the decoder's output which is a 3-rank tensor\n",
        "    with shape [batch_size, sequence_length, num_words]\n",
        "    so that for each sequence in the batch there is a one-hot\n",
        "    encoded array of length num_words.\n",
        "    \"\"\"\n",
        "\n",
        "    # Calculate the loss. This outputs a\n",
        "    # 2-rank tensor of shape [batch_size, sequence_length]\n",
        "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true,\n",
        "                                                          logits=y_pred)\n",
        "\n",
        "    # Keras may reduce this across the first axis (the batch)\n",
        "    # but the semantics are unclear, so to be sure we use\n",
        "    # the loss across the entire 2-rank tensor, we reduce it\n",
        "    # to a single scalar with the mean function.\n",
        "    loss_mean = tf.reduce_mean(loss)\n",
        "\n",
        "    return loss_mean"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZIYHe1WuPcU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = RMSprop(lr=1e-3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVoSSZp0uSWz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder_target = tf.placeholder(dtype='int32', shape=(None, None))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIeYLbAtuUnO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_train.compile(optimizer=optimizer,\n",
        "                    loss=sparse_cross_entropy,\n",
        "                    target_tensors=[decoder_target])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0nR8QDguXGg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_checkpoint = '21_checkpoint.keras'\n",
        "callback_checkpoint = ModelCheckpoint(filepath=path_checkpoint,\n",
        "                                      monitor='val_loss',\n",
        "                                      verbose=1,\n",
        "                                      save_weights_only=True,\n",
        "                                      save_best_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfIkZKpSuZyd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "callback_early_stopping = EarlyStopping(monitor='val_loss',\n",
        "                                        patience=3, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uO05ZbGGucVg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "callback_tensorboard = TensorBoard(log_dir='./21_logs/',\n",
        "                                   histogram_freq=0,\n",
        "                                   write_graph=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QBO9k87ue1M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "callbacks = [callback_early_stopping,\n",
        "             callback_checkpoint,\n",
        "             callback_tensorboard]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kokw_l6zug9s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c3107ab8-9fca-4ae9-ce8a-7f0ed33b1658"
      },
      "source": [
        "try:\n",
        "    model_train.load_weights(path_checkpoint)\n",
        "except Exception as error:\n",
        "    print(\"Error trying to load checkpoint.\")\n",
        "    print(error)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Error trying to load checkpoint.\n",
            "Unable to open file (unable to open file: name = '21_checkpoint.keras', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAKDIblNujn-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_data = \\\n",
        "{\n",
        "    'encoder_input': encoder_input_data,\n",
        "    'decoder_input': decoder_input_data\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFbjKzw7vmc2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_data = \\\n",
        "{\n",
        "    'decoder_output': decoder_output_data\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znopxvpcvrsp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "129d9dc6-b6e8-40dd-f151-30c69076d9cc"
      },
      "source": [
        "validation_split = 10000 / len(encoder_input_data)\n",
        "validation_split"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0050792360828931325"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cuf62UvVvuko",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "6735375b-972c-474a-fa61-c03f9cdecd28"
      },
      "source": [
        "model_train.fit(x=x_data,\n",
        "                y=y_data,\n",
        "                batch_size=512,\n",
        "                epochs=1,\n",
        "                validation_split=validation_split,\n",
        "                callbacks=callbacks)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Train on 1958800 samples, validate on 10000 samples\n",
            "1958400/1958800 [============================>.] - ETA: 0s - loss: 2.1072\n",
            "Epoch 00001: val_loss improved from inf to 1.69093, saving model to 21_checkpoint.keras\n",
            "1958800/1958800 [==============================] - 2364s 1ms/sample - loss: 2.1071 - val_loss: 1.6909\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f54bc5d8898>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SI5wFxdvxny",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate(input_text, true_output_text=None):\n",
        "    \"\"\"Translate a single text-string.\"\"\"\n",
        "\n",
        "    # Convert the input-text to integer-tokens.\n",
        "    # Note the sequence of tokens has to be reversed.\n",
        "    # Padding is probably not necessary.\n",
        "    input_tokens = tokenizer_src.text_to_tokens(text=input_text,\n",
        "                                                reverse=True,\n",
        "                                                padding=True)\n",
        "    \n",
        "    # Get the output of the encoder's GRU which will be\n",
        "    # used as the initial state in the decoder's GRU.\n",
        "    # This could also have been the encoder's final state\n",
        "    # but that is really only necessary if the encoder\n",
        "    # and decoder use the LSTM instead of GRU because\n",
        "    # the LSTM has two internal states.\n",
        "    initial_state = model_encoder.predict(input_tokens)\n",
        "\n",
        "    # Max number of tokens / words in the output sequence.\n",
        "    max_tokens = tokenizer_dest.max_tokens\n",
        "\n",
        "    # Pre-allocate the 2-dim array used as input to the decoder.\n",
        "    # This holds just a single sequence of integer-tokens,\n",
        "    # but the decoder-model expects a batch of sequences.\n",
        "    shape = (1, max_tokens)\n",
        "    decoder_input_data = np.zeros(shape=shape, dtype=np.int)\n",
        "\n",
        "    # The first input-token is the special start-token for 'ssss '.\n",
        "    token_int = token_start\n",
        "\n",
        "    # Initialize an empty output-text.\n",
        "    output_text = ''\n",
        "\n",
        "    # Initialize the number of tokens we have processed.\n",
        "    count_tokens = 0\n",
        "\n",
        "    # While we haven't sampled the special end-token for ' eeee'\n",
        "    # and we haven't processed the max number of tokens.\n",
        "    while token_int != token_end and count_tokens < max_tokens:\n",
        "        # Update the input-sequence to the decoder\n",
        "        # with the last token that was sampled.\n",
        "        # In the first iteration this will set the\n",
        "        # first element to the start-token.\n",
        "        decoder_input_data[0, count_tokens] = token_int\n",
        "\n",
        "        # Wrap the input-data in a dict for clarity and safety,\n",
        "        # so we are sure we input the data in the right order.\n",
        "        x_data = \\\n",
        "        {\n",
        "            'decoder_initial_state': initial_state,\n",
        "            'decoder_input': decoder_input_data\n",
        "        }\n",
        "\n",
        "        # Note that we input the entire sequence of tokens\n",
        "        # to the decoder. This wastes a lot of computation\n",
        "        # because we are only interested in the last input\n",
        "        # and output. We could modify the code to return\n",
        "        # the GRU-states when calling predict() and then\n",
        "        # feeding these GRU-states as well the next time\n",
        "        # we call predict(), but it would make the code\n",
        "        # much more complicated.\n",
        "\n",
        "        # Input this data to the decoder and get the predicted output.\n",
        "        decoder_output = model_decoder.predict(x_data)\n",
        "\n",
        "        # Get the last predicted token as a one-hot encoded array.\n",
        "        token_onehot = decoder_output[0, count_tokens, :]\n",
        "        \n",
        "        # Convert to an integer-token.\n",
        "        token_int = np.argmax(token_onehot)\n",
        "\n",
        "        # Lookup the word corresponding to this integer-token.\n",
        "        sampled_word = tokenizer_dest.token_to_word(token_int)\n",
        "\n",
        "        # Append the word to the output-text.\n",
        "        output_text += \" \" + sampled_word\n",
        "\n",
        "        # Increment the token-counter.\n",
        "        count_tokens += 1\n",
        "\n",
        "    # Sequence of tokens output by the decoder.\n",
        "    output_tokens = decoder_input_data[0]\n",
        "    \n",
        "    # Print the input-text.\n",
        "    print(\"Input text:\")\n",
        "    print(input_text)\n",
        "    print()\n",
        "\n",
        "    # Print the translated output-text.\n",
        "    print(\"Translated text:\")\n",
        "    print(output_text)\n",
        "    print()\n",
        "\n",
        "    # Optionally print the true translated text.\n",
        "    if true_output_text is not None:\n",
        "        print(\"True output text:\")\n",
        "        print(true_output_text)\n",
        "        print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeDUkSDqwusE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "d4787bb8-5092-4d6f-a201-e4fa7451ac81"
      },
      "source": [
        "idx = 3\n",
        "translate(input_text=data_src[idx],\n",
        "          true_output_text=data_dest[idx])"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input text:\n",
            "De har udtrykt ønske om en debat om dette emne i løbet af mødeperioden.\n",
            "\n",
            "Translated text:\n",
            " you have a debate on this issue in the debate on the issue of the debate eeee\n",
            "\n",
            "True output text:\n",
            "ssss You have requested a debate on this subject in the course of the next few days, during this part-session. eeee\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-Q2aXIAoE5t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "e321e773-be7e-4d5d-a812-83c99c2ed159"
      },
      "source": [
        "\n",
        "idx = 3\n",
        "translate(input_text=data_src[idx] + data_src[idx+1],\n",
        "          true_output_text=data_dest[idx] + data_dest[idx+1])"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input text:\n",
            "De har udtrykt ønske om en debat om dette emne i løbet af mødeperioden.I mellemtiden ønsker jeg - som også en del kolleger har anmodet om - at vi iagttager et minuts stilhed til minde om ofrene for bl.a. stormene i de medlemslande, der blev ramt.\n",
            "\n",
            "Translated text:\n",
            " you have been a member of the committee on petitions to put forward a debate on this issue i would like to say that we have been able to speak on the issue of the situation in the middle east in the united states eeee\n",
            "\n",
            "True output text:\n",
            "ssss You have requested a debate on this subject in the course of the next few days, during this part-session. eeeessss In the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union. eeee\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "089S07rWoFla",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "4619d0ba-55aa-49af-e447-f457d21f8c7f"
      },
      "source": [
        "idx = 3\n",
        "translate(input_text=data_src[idx+1] + data_src[idx],\n",
        "          true_output_text=data_dest[idx+1] + data_dest[idx])"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input text:\n",
            "I mellemtiden ønsker jeg - som også en del kolleger har anmodet om - at vi iagttager et minuts stilhed til minde om ofrene for bl.a. stormene i de medlemslande, der blev ramt.De har udtrykt ønske om en debat om dette emne i løbet af mødeperioden.\n",
            "\n",
            "Translated text:\n",
            " i would also like to say that we have been able to speak on the issue of the people of the european union in the debate that we have been able to find a solution to the problem of the people of the european union in the event of a referendum on the issue of\n",
            "\n",
            "True output text:\n",
            "ssss In the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union. eeeessss You have requested a debate on this subject in the course of the next few days, during this part-session. eeee\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJtsOf92oL3S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "88b6f935-3312-41cf-cc93-26f5d0a0f98f"
      },
      "source": [
        "translate(input_text=\"der var engang et land der hed Danmark\",\n",
        "          true_output_text='Once there was a country named Denmark')"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input text:\n",
            "der var engang et land der hed Danmark\n",
            "\n",
            "Translated text:\n",
            " there was a risk of a war in the united states eeee\n",
            "\n",
            "True output text:\n",
            "Once there was a country named Denmark\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUTAq_CIoRL-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "305694dd-bd22-4d7e-bf17-f90f3c323a27"
      },
      "source": [
        "translate(input_text=\"Idag kan man læse i avisen at Danmark er blevet fornuftigt\",\n",
        "          true_output_text=\"Today you can read in the newspaper that Denmark has become sensible.\")"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input text:\n",
            "Idag kan man læse i avisen at Danmark er blevet fornuftigt\n",
            "\n",
            "Translated text:\n",
            " if the question of the question of the death penalty was then the case of the eeee\n",
            "\n",
            "True output text:\n",
            "Today you can read in the newspaper that Denmark has become sensible.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEh1RkKOoZAf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "2b64aeeb-55aa-422a-8e23-6a457bb77fea"
      },
      "source": [
        "translate(input_text=\"Hvem spæner ud af en butik og tygger de stærkeste bolcher?\",\n",
        "          true_output_text=\"Who runs out of a shop and chews the strongest bon-bons?\")"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input text:\n",
            "Hvem spæner ud af en butik og tygger de stærkeste bolcher?\n",
            "\n",
            "Translated text:\n",
            " what is the people of the member states and the commission eeee\n",
            "\n",
            "True output text:\n",
            "Who runs out of a shop and chews the strongest bon-bons?\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yui1xr54oei2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}